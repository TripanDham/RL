{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A bunch of imports, you don't have to worry about these\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import gym\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "import tensorflow as tf\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "import tensorflow_probability as tfp\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: rcoh67u0\n",
      "Sweep URL: https://wandb.ai/tripan-dham/dueling_mean_cartpole/sweeps/rcoh67u0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 18214b26 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH_SIZE: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBUFFER_SIZE: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLR: 0.006728480086476333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUPDATE_EVERY: 100\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\tripa\\AppData\\Local\\Programs\\Microsoft VS Code\\wandb\\run-20240404_025514-18214b26</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/18214b26/workspace' target=\"_blank\">gentle-sweep-1</a></strong> to <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/sweeps/rcoh67u0' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole/sweeps/rcoh67u0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/sweeps/rcoh67u0' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole/sweeps/rcoh67u0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/18214b26/workspace' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/18214b26/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f173a8312a3f412aa730e29cdea5dd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.005 MB uploaded\\r'), FloatProgress(value=0.22918272937548187, max=1.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-1</strong> at: <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/18214b26/workspace' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/18214b26/workspace</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240404_025514-18214b26\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 18214b26 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tripa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 256, in run_training\n",
      "    regret = train(LR, UPDATE_EVERY, BUFFER_SIZE, BATCH_SIZE)\n",
      "  File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 239, in train\n",
      "    all_scores_1=dqn(agent)\n",
      "  File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 176, in dqn\n",
      "    state,_ = env.reset()\n",
      "NameError: name 'env' is not defined\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 18214b26 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\tripa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 256, in run_training\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     regret = train(LR, UPDATE_EVERY, BUFFER_SIZE, BATCH_SIZE)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 239, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     all_scores_1=dqn(agent)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 176, in dqn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     state,_ = env.reset()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'env' is not defined\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: clgii36t with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBUFFER_SIZE: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLR: 0.004760347066776705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUPDATE_EVERY: 20\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\tripa\\AppData\\Local\\Programs\\Microsoft VS Code\\wandb\\run-20240404_025537-clgii36t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/clgii36t/workspace' target=\"_blank\">summer-sweep-2</a></strong> to <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/sweeps/rcoh67u0' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole/sweeps/rcoh67u0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/sweeps/rcoh67u0' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole/sweeps/rcoh67u0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/clgii36t/workspace' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/clgii36t/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56545db8382e4665a822a6662a8d927f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.005 MB uploaded\\r'), FloatProgress(value=0.2302924656207631, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-sweep-2</strong> at: <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/clgii36t/workspace' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/clgii36t/workspace</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240404_025537-clgii36t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run clgii36t errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tripa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 256, in run_training\n",
      "    regret = train(LR, UPDATE_EVERY, BUFFER_SIZE, BATCH_SIZE)\n",
      "  File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 239, in train\n",
      "    all_scores_1=dqn(agent)\n",
      "  File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 176, in dqn\n",
      "    state,_ = env.reset()\n",
      "NameError: name 'env' is not defined\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run clgii36t errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\tripa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 256, in run_training\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     regret = train(LR, UPDATE_EVERY, BUFFER_SIZE, BATCH_SIZE)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 239, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     all_scores_1=dqn(agent)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 176, in dqn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     state,_ = env.reset()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'env' is not defined\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p9ylx7rh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBUFFER_SIZE: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLR: 0.0025869369020605653\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUPDATE_EVERY: 50\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\tripa\\AppData\\Local\\Programs\\Microsoft VS Code\\wandb\\run-20240404_025549-p9ylx7rh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/p9ylx7rh/workspace' target=\"_blank\">tough-sweep-3</a></strong> to <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/sweeps/rcoh67u0' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole/sweeps/rcoh67u0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/sweeps/rcoh67u0' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole/sweeps/rcoh67u0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/p9ylx7rh/workspace' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/p9ylx7rh/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a1fd8da8874bcda8004b03a8925db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.005 MB uploaded\\r'), FloatProgress(value=0.2303077220824463, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-sweep-3</strong> at: <a href='https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/p9ylx7rh/workspace' target=\"_blank\">https://wandb.ai/tripan-dham/dueling_mean_cartpole/runs/p9ylx7rh/workspace</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240404_025549-p9ylx7rh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run p9ylx7rh errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tripa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 256, in run_training\n",
      "    regret = train(LR, UPDATE_EVERY, BUFFER_SIZE, BATCH_SIZE)\n",
      "  File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 239, in train\n",
      "    all_scores_1=dqn(agent)\n",
      "  File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 176, in dqn\n",
      "    state,_ = env.reset()\n",
      "NameError: name 'env' is not defined\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run p9ylx7rh errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\tripa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 256, in run_training\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     regret = train(LR, UPDATE_EVERY, BUFFER_SIZE, BATCH_SIZE)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 239, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     all_scores_1=dqn(agent)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\tripa\\AppData\\Local\\Temp\\ipykernel_23720\\679867947.py\", line 176, in dqn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     state,_ = env.reset()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'env' is not defined\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Bunch of Hyper parameters (Which you might have to tune later)\n",
    "'''\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 50   # how often to update the network (When Q target is present)\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "class QNetwork1(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64, fc3_units=64, fc4_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork1, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        # Advantage stream\n",
    "        self.fc3 = nn.Linear(state_size, fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units, fc4_units)\n",
    "\n",
    "        self.fc_advantage = nn.Linear(fc4_units, action_size)\n",
    "        # Value stream\n",
    "        self.fc_value = nn.Linear(fc2_units, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.fc_value(x)\n",
    "        x = F.relu(self.fc3(state))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        advantage = self.fc_advantage(x)\n",
    "\n",
    "        # Combine value and advantage to get Q-values\n",
    "        Q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "\n",
    "        return Q_values\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "class TutorialAgent1():\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "\n",
    "        ''' Agent Environment Interaction '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        ''' Q-Network '''\n",
    "        self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        ''' Replay memory '''\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "\n",
    "        ''' Save experience in replay memory '''\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        ''' If enough samples are available in memory, get random subset and learn '''\n",
    "        if len(self.memory) >= BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
    "        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "\n",
    "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        ''' Epsilon-greedy action selection (Already Present) '''\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        Q_targets_next = self.qnetwork_target(next_states)\n",
    "\n",
    "        # Compute value and advantage streams\n",
    "        next_state_values = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * next_state_values * (1 - dones))\n",
    "\n",
    "        # Compute Q-values for current states using local network\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "        ''' Defining DQN Algorithm '''\n",
    "\n",
    "def dqn(agent,n_episodes=10000, max_t=500, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "\n",
    "    scores_window = deque(maxlen=100)\n",
    "    all_scores=np.array([])\n",
    "    moving_avg_scores=np.array([])\n",
    "    done,truncated=False,False\n",
    "    ''' last 100 scores for checking if the avg is more than 195 '''\n",
    "\n",
    "    eps = eps_start\n",
    "    ''' initialize epsilon '''\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state,_ = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state,reward, done, truncated,_ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done or truncated:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        all_scores=np.append(all_scores,score)\n",
    "\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        ''' decrease epsilon '''\n",
    "\n",
    "        moving_avg_scores=np.append(moving_avg_scores,np.mean(scores_window))\n",
    "\n",
    "  \n",
    "        if i_episode==500:\n",
    "           break\n",
    "    return moving_avg_scores,True\n",
    "\n",
    "def regret(all1):\n",
    "  regret=0\n",
    "  for i in all1:\n",
    "    if i>500:\n",
    "      break\n",
    "    else:\n",
    "      regret+=500-i\n",
    "  return regret\n",
    "\n",
    "sweep_config = {\n",
    "\"method\": \"bayes\",\n",
    "\"metric\": {\"name\": \"regret\", \"goal\": \"minimize\"},\n",
    "\"parameters\": {\n",
    "    \"LR\": {\"min\": 1e-5, \"max\": 1e-2},\n",
    "    \"UPDATE_EVERY\": {\"values\":[20,50,75,100]},\n",
    "    \"BUFFER_SIZE\": {\"values\":[1e2,1e3,1e5]},\n",
    "    \"BATCH_SIZE\": {\"values\":[64,128,256]}\n",
    "},\n",
    "\"project\": \"dueling_mean_cartpole\",\n",
    "\"early_terminate\": {\n",
    "    \"type\": \"hyperband\",\n",
    "    \"min_iter\": 3,\n",
    "    \"max_iter\": 100\n",
    "}\n",
    "}\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep_config)\n",
    "\n",
    "def train(LR, UPDATE_EVERY, BUFFER_SIZE, BATCH_SIZE):\n",
    "    UPDATE_EVERY = UPDATE_EVERY\n",
    "    BUFFER_SIZE = BUFFER_SIZE\n",
    "    LR = LR\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_shape = env.observation_space.shape[0]\n",
    "    action_shape = env.action_space.n\n",
    "    no_of_actions = env.action_space.n\n",
    "    state,_ = env.reset()\n",
    "    state=np.array(state)\n",
    "    agent = TutorialAgent1(state_size=state_shape,action_size = action_shape,seed = 0)\n",
    "    all_scores_1=dqn(agent)\n",
    "    all1=(all_scores_1[0])\n",
    "    regret = regret(all1)\n",
    "    return regret\n",
    "\n",
    "def run_training():\n",
    "    config_defaults = {\n",
    "        \"LR\": 5e-4,\n",
    "        \"UPDATE_EVERY\": 50,\n",
    "        \"BUFFER_SIZE\": 1e5,\n",
    "        \"BATCH_SIZE\": 64\n",
    "    }\n",
    "    config = wandb.init(config=config_defaults,project=\"dueling_mean_cartpole\")\n",
    "    LR = config.config[\"LR\"]\n",
    "    UPDATE_EVERY=config.config['UPDATE_EVERY']\n",
    "    BATCH_SIZE = config.config[\"BATCH_SIZE\"]\n",
    "    BUFFER_SIZE = config.config[\"BUFFER_SIZE\"]\n",
    "    regret = train(LR, UPDATE_EVERY, BUFFER_SIZE, BATCH_SIZE)\n",
    "    wandb.log({\"regret\": regret})\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, function=run_training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
